# 220928 Diffusion Models
Diffusion Model은 특정한 입력 값이 여러 단계에 걸쳐 순수 가우시안 확률을 갖는 노이즈 형태로 파괴되는 과정을 수학적으로 정의하고, 그 역방향의 전이과정을 딥러닝을 통해 추정하는 모델입니다.

각 단계를 정의 하기 전에, diffusion model은 각 단계를 Markov chain으로 가정하여 특정 시점(t)의 latent variable은 바로 직전 시점의 latent variable만 연관 되고 나머지는 무시하는 방식을 활용합니다.

따라서 diffusion model을 정의하기 위해선 확산에 의한 전이 모델을 설계하고, 완전히 파괴된 입력 정보를 역방향으로 전이 시켜가며 변화하는 확률 모델을 통해 loss 함수를 설계해야 합니다.

## Diffusion Process
확산 모델은 기본적으로는 $q\left(x_t \vert x_{t-1}, x_{t-1}, ..., x_0\right)$의 형태로 정의 할 수 있습니다. 이 때 확산 과정은 Markov chain을 가정하였으므로 다음과 같이 단순화 하여 적을 수 있습니다.
$$
\begin{equation}
q\left(x_t \vert x_{t-1}\right)
\end{equation}
$$

또한 확산 과정은 가우시안 프로세스를 따른다는 가정을 통해 결과적으로 입력 $x_0$가 주어졌을 때, 구간 $\begin{bmatrix}0 & T\end{bmatrix}$에서의 전이 과정은 다음과 같이 정의할 수 있습니다.
$$
\begin{equation}
\begin{matrix}
q\left(x_t \vert x_{t-1}\right) = \mathcal{N}\left(x_t; \sqrt{1-{\beta_t}} x_{t-1}, \beta_t \bold{I}\right) &
q\left(x_{1:T} \vert x_0\right) = \prod_{t=1}^{T}q\left(x_t \vert x_{t-1}\right)
\end{matrix}
\end{equation}
$$

t-1 단계에서 t 단계로 전이되는 과정은 가우시안 분포를 따르기 때문에 단계적으로는 아래와 같이 정의할 수 있습니다.
$$
\begin{equation}
\begin{aligned}
x_1 &= \sqrt{\alpha_1}x_0 + \sqrt{1-\alpha_1}\epsilon_0 \\
x_2 &= \sqrt{\alpha_2}x_1 + \sqrt{1-\alpha_2}\epsilon_1 \\
...\\
x_T &= \sqrt{\alpha_T}x_{T-1} + \sqrt{1-\alpha_T}\epsilon_{T-1} \\
&\text{where } \alpha_t = 1-\beta_t \text{, and } \epsilon_t \sim \mathcal{N}\left(0, I\right)
\end{aligned}
\end{equation}
$$

이때 각 단계에서 전이 결과를 학습 과정에서 사용하기 위해서는, 원본 이미지가 주어졌을 때 t 단계의 결과를 빠르게 확인 할 수 있어야 합니다. 따라서 각각의 식을 원래 입력 $x_0$에 대한 식으로 바꾸는 과정은 다음과 같습니다.
$$
\begin{equation}
\begin{aligned}
x_t &= \sqrt{\alpha_t}x_{t-1} + \sqrt{1-\alpha_t}\epsilon_{t-1} \\
 &= \sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_{t-1}}\epsilon_{t-2}) + \sqrt{1-\alpha_t}\epsilon_{t-1} \\
  &=\sqrt{{\alpha_t}{\alpha_{t-1}}}x_{t-2} + \sqrt{\alpha_{t}(1-\alpha_{t-1})}\epsilon_{t-2} + \sqrt{1-\alpha_t}\epsilon_{t-1}
\end{aligned}
\end{equation}
$$

우선 위의 식에서 $\epsilon_{t-2}$와 $\epsilon_{t-1}$의 합성에 대해 먼저 살펴보면, 각각은 정규분포를 따르는 랜덤 벡터입니다. 각 단계에서의 변수 $x$들은 단순한 형태로 계산이 가능해 보이지만 $\epsilon$의 경우 여러 단계의 값을 모두 더해가는 과정을 포함합니다. 따라서 이를 간단한 방식으로 풀어내야 합니다. 이를 위해 두개의 독립적인 랜덤 벡터 $z_1$, $z_2$가 주어지고, 각각은 공분산이 $\Sigma_1$, $\Sigma_2$, 평균은 모두 0인 가우시안 분포를 따른다고 가정해봅니다. 이러한 경우 두 랜덤 벡터의 합은 어떤 평균과 공분산을 만드는 지 확인해보겠습니다. 평균의 경우 두 랜덤 벡터 모두 0이므로 간단히 0으로 정의할 수 있습니다. 따라서 공분산만 구할 수 있다면 두 랜덤 벡터 합의 공분산이 무엇인지 알 수 있어야합니다.
$$
\begin{equation}
\begin{aligned}
\mathbb{Cov} \begin{bmatrix}\epsilon_1 + \epsilon_2\end{bmatrix} &= \mathbb{E}\begin{bmatrix}(\epsilon_1 + \epsilon_2)(\epsilon_1 + \epsilon_2)^T\end{bmatrix} \\
&= \mathbb{E}\begin{bmatrix}\epsilon_1\epsilon_1^T + \epsilon_1\epsilon_2^T + \epsilon_2\epsilon_1^T +  \epsilon_2\epsilon_2^T\end{bmatrix} \\
&= \mathbb{E}\begin{bmatrix}\epsilon_1\epsilon_1^T\end{bmatrix} + \mathbb{E}\begin{bmatrix}\epsilon_1\epsilon_2^T\end{bmatrix} + \mathbb{E}\begin{bmatrix}\epsilon_2\epsilon_1^T\end{bmatrix} +  \mathbb{E}\begin{bmatrix}\epsilon_2\epsilon_2^T\end{bmatrix} \\
&= \Sigma_1 + \underbrace{\mathbb{E}\begin{bmatrix}\epsilon_1\epsilon_2^T\end{bmatrix}}_{indep. \rightarrow =0} + \underbrace{\mathbb{E}\begin{bmatrix}\epsilon_2\epsilon_1^T\end{bmatrix}}_{indep. \rightarrow =0} +  \Sigma_2 \\
&= \Sigma_1 + \Sigma_2
\end{aligned}
\end{equation}
$$
따라서 가우시안 분포를 따르는 두개의 독립적인 랜덤 벡터가 있을 때 두 랜덤 벡터의 합성은 공분산을 더해주기만 하면 됩니다. 따라서 식(4)를 추가로 정리하면 다음과 같습니다.
$$
\begin{equation}
\begin{aligned}
x_t &=\sqrt{{\alpha_t}{\alpha_{t-1}}}x_{t-2} + \sqrt{\alpha_{t}(1-\alpha_{t-1})}\epsilon_{t-2} + \sqrt{1-\alpha_t}\epsilon_{t-1} \\
&=\sqrt{{\alpha_t}{\alpha_{t-1}}}x_{t-2} + \sqrt{\alpha_{t}(1-\alpha_{t-1}) + 1-\alpha_t}\epsilon_{t-2}^{\prime} \\
&=\sqrt{{\alpha_t}{\alpha_{t-1}}}x_{t-2} + \sqrt{1-\alpha_t\alpha_{t-1}}\epsilon_{t-2}^{\prime} \\
\end{aligned}
\end{equation}
$$
여기서 $\epsilon^{\prime}$은 단순 이전 $\epsilon$과 다른 랜덤 벡터라는 의미로 사용되었습니다.
이를 기반으로 $x_0$에 관한 식이 될 때 까지 식을 전개하면 다음과 같습니다.
$$
\begin{equation}
\begin{aligned}
x_t &=\sqrt{{\alpha_t}{\alpha_{t-1}}}x_{t-2} + \sqrt{1-\alpha_t\alpha_{t-1}}\epsilon_{t-2}^{\prime} \\
 &=\sqrt{{\alpha_t}{\alpha_{t-1}}}(\sqrt{\alpha_{t-2}}x_{t-3} + \sqrt{1-\alpha_{t-2}}\epsilon_{t-3})) + \sqrt{1-\alpha_t\alpha_{t-1}}\epsilon_{t-2}^{\prime} \\
 &=\sqrt{{\alpha_t}{\alpha_{t-1}}\alpha_{t-2}}x_{t-3} + \sqrt{{\alpha_t}{\alpha_{t-1}}}\sqrt{1-\alpha_{t-2}}\epsilon_{t-3} + \sqrt{1-\alpha_t\alpha_{t-1}}\epsilon_{t-2}^{\prime} \\
 &=\sqrt{{\alpha_t}{\alpha_{t-1}}\alpha_{t-2}}x_{t-3} + \sqrt{1-{\alpha_t}{\alpha_{t-1}}\alpha_{t-2}}\epsilon_{t-3}^{\prime} \\
  &...\\
  &=x_0\prod_{s=1}^{t}\alpha_s + \epsilon\sqrt{1 - \prod_{s=1}^{t}\alpha_s} \\
  &= \bar{\alpha}_{t}x_0 + \epsilon\sqrt{1 - \bar{\alpha}_{t}}\\
  &\text{where } \epsilon \sim \mathcal{N}(0,I) \text{, } \bar{\alpha}_t=\prod_{s=1}^{t}\alpha_s \text{ and } \alpha_t = 1-\beta_t
\end{aligned}
\end{equation}
$$

## Reverse Process
역방향 프로세스는 diffusion model에서 입력 이미지가 확산을 통해 순수 가우시안 노이즈 형태로 파괴 되어가는 과정을 역으로 되돌리는 것 입니다. 즉, t번 확산 단계를 거친 이미지가 주어졌을 때 t-1번 확산 단계를 거친 이미지의 형태를 추정하는 프로세스 입니다. 따라서 이를 확산 과정과 비교해 보면 다음과 같습니다.
$$
\begin{equation}
\begin{aligned}
q(x_t \vert x_{t-1}) &= \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_tI)\\
p_{\theta}(x_{t-1} \vert x_{t}) &= \mathcal{N}(x_{t-1}; \mu_{\theta}(x_{t},t), \Sigma_{\theta}(x_t, t))
\end{aligned}
\end{equation}
$$
$q(x_t \vert x_{t-1})$ 은 앞서 살펴본 확산 과정에 대한 확률 모델이고, $p_{\theta}(x_{t-1} \vert x_t)$가 역방향 과정에 관한 확률 모델이다. 역방향 과정 모델의 아랫첨자로 붙은 $\theta$는 학습 파라미터로 구성된 모델이라는 의미이다. DDPM(Denoising Diffusion Probabilistic Model, 2020, Jonathan Ho, et. al.)의 저자들은 $\mu_{\theta}$와 $\Sigma_{\theta}$로 구성된 학습 파라미터들 중에 $\Sigma_{\theta}$는 $\sigma_t^2I$로 두어 학습하지 않는 시간 종속적인 상수로 설정해 두었다. 이는 $x_0 \sim \mathcal{N}(0, I)$에 대해 최적이다.

## Loss
최종적으로 학습을 하기 위해서는 손실 함수에 대해 정의해야 한다. 입력 $x_0$로부터 출발한 모델은 확산 단계와 역방향 단계를 거쳐 다시 $x_0$로 되돌아 오게 되는데, 입력과 출력 사이의 차이를 정의 하면 다음과 같다.
$$
\begin{equation}
\begin{aligned}
L_{CE}=-\mathbb{E}_{q(x_0)}\left[p_{\theta}(x_0)\right]=-\mathbb{E}_{q(x_0)}\left[\int{p_{\theta}(x_{0:T})dx_{1:T}}\right]\\
\end{aligned}
\end{equation}